{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import re\n",
    "from rapidfuzz import fuzz, process\n",
    "import os\n",
    "import json\n",
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examing a random page in the book to see the structure and quality of the extracted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EGGS  175 \n",
      "stove;  break  the  eggs  into  the  boiling  sauce,  season  with  a \n",
      "grain  of  salt,  and  put  the  cocottes,  one  by  one,  into  a  saute-pan \n",
      "containing  the  necessary  quantity  of  boiling  water. \n",
      "Poach  as  directed,  and  set  to  glaze  quickly  at  the  last \n",
      "moment. \n",
      "442— EQGS  EN  COCOTTE  WITH  CREAM \n",
      "This  preparation  constitutes  the  radical  type  of  this  series \n",
      "of  eggs,  and,  for  a  long  time,  was  the  only  one  in  use.  Heat \n",
      "the  cocottes  beforehand;  pour  a  tablespoonful  of  boiling  cream \n",
      "into  each,  followed  by  an  egg,  broken ;  season,  and  add  two \n",
      "little  lumps  of  butter,  the  size  of  peas.  Place  the  cocottes  in \n",
      "a  bain-marie,  and  poach  as  before. \n",
      "443— EaaS  EN  COCOTTE  A  LA  JEANNETTE \n",
      "Garnish  the  bottom  and  the  sides  of  the  cocottes  with  a \n",
      "thickness  of  one-third  inch  of  chicken-forcemeat  with  cream, \n",
      "mixed  with  a  fifth  of  its  volume  of  foie  gras.  Break  the  egg \n",
      "over  the  middle,  season,  and  poach  in  the  usual  way.  When \n",
      "about  to  serve,  surround  the  eggs  with  a  thread  of  poultry \n",
      "velout^. \n",
      "444— EQGS  EN  COCOTTE  WITH  GRAVY \n",
      "Break  the  eggs  into  buttered  cocottes.  Season,  poach,  and, \n",
      "when  about  to  serve,  surround  the  yolks  with  a  thread  of \n",
      "reduced  veal  gravy. \n",
      "445— EGGS  EN  COCOTTE  A  LA  LORRAINE \n",
      "Put  a  teaspoonful  of  breast  of  pork,  cut  into  dice  and  fried, \n",
      "into  each  cocotte,  also  three  thin  slices  of  Gruy^re  cheese  and \n",
      "one  tablespoonful  of  boiling  cream.  Break  the  eggs,  season, \n",
      "and  poach  in  the  usual  way. \n",
      "446— EQGS  EN  COCOTTE  A  LA  MARAICHERE \n",
      "Garnish  the  bottom  and  sides  of  the  cocottes  with  cooked \n",
      "spinach,  chopped  and  pressed,  and  sorrel  and  lettuce  leaves, \n",
      "both  of  which  should  be  stewed  in  butter.  Break  the  eggs, \n",
      "season,  poach  in  the  usual  way,  and,  when  about  to  send  the \n",
      "eggs  to  the  table,  drop  a  fine  chervil  -pluche  on  each  yolk. \n",
      "447— EGGS  EN  COCOTTE  WITH  MORELS \n",
      "Garnish  the  bottom  and  sides  of  the  cocottes  with  minced \n",
      "morels  fried  in  butter  and  thickened  with  a  little  reduced  half- \n",
      "glaze.  Break  the  eggs,  season,  poach,  and  surround  the  yolks \n",
      "with  a  thread  of  half-glaze  when  dishing  up. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = pymupdf.open(\"a guide to modern cookery.pdf\")\n",
    "out = open(\"output.txt\", \"wb\")\n",
    "i = 0\n",
    "for page in doc:\n",
    "    i += 1\n",
    "    if i > 200:\n",
    "        text = page.get_text()#.encode(\"utf8\")\n",
    "        # out.write(text)\n",
    "    if i == 201:\n",
    "        print(text)\n",
    "    if i == 230:\n",
    "        break \n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic, the data seems to be well extracted and follow a consistent structure. Recipe titles are in all caps followed by instructions. There are some mistakes due to whatever OCR was originally used but they appear to be minor (e.g. \"EQGS\" instead of \"EGGS\".)\n",
    "\n",
    "Now I'll open the PDF and examine it manually:\n",
    "-pages 1-22 are the preface and table of contents.\n",
    "- The glossary could be useful but is structured differently and not all entries are useful. I'll ignore it for now since most entries are elaborated on later.\n",
    "- Each part appears to have a brief introduction before following the structure seen above. \n",
    "- Given that the page numbers of the book begin at PDF page 27, I'll extract each chapter separately to label their metadata according to the chapter title (e.g. \"Stocks\", \"Sauces\", \"Soups\", etc.) without having to iterate through the entire 1000 page PDF immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the table of contents to get the titles and page numbers for each chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"CONTENTS \\nPART    I \\nFUNDAMENTAL   ELEMENTS \\nCHAPTER  I \\nPAGE \\nFONDS  DE  CUISINE  ........  I \\nCHAPTER  II \\nTHE  LEADING  WARM   SAUCES     .....  \\xe2\\x80\\xa2  '5 \\nCHAPTER  III \\nTHE   SMALL  COMPOUND   SAUCES  ...  .  .  24 \\nCHAPTER  IV \\nCOLD  SAUCES  AND  COMPOUND  BUTTERS        .....  48 \\nCHAPTER  V \\nSAVOURY  JELLIES  OR  ASPICS  .  ......  59 \\nCHAPTER  VI \\nTHE  COURT-BOUILLONS  AND  THE  MARINADES  .  .  .  -64 \\nCHAPTER  VII \\n\\\\J/:  ELEMENTARY  PREPARATIONS  .....  70 \\nCHAPTER  VIII \\nTHE  VARIOUS  GARNISHES   FOR  SOUPS  .  .  .  .  87 \\nCHAPTER  IX \\nGARNISHING  PREPARATIONS   FOR  RELEVis  AND   ENTR]\\xc2\\xa3eS  .  .  92 \\nCHAPTER  X \\nU^DING  CULINARY  OPERATIONS  .  ....  97 \\n\"\n",
      "b\"xii  CONTENTS \\nPART   II \\nRECIPES  AND   MODES   OF  PROCEDURE \\nCHAPTER  XI \\nPAGE \\nHORS-D'CEUVRES      .  .  .  .  .  .  .  ,  .137 \\nCHAPTER  XII \\nEGGS  .......  .  .        164 \\nCHAPTER  XIII \\nSOUPS  ..........      197 \\nCHAPTER  XIV \\nFISH  ..........        260 \\nCHAPTER  XV \\nRELEVilS  AND  ENTRIES  OF  BUTCHER'S  MEAT  ....       352 \\nCHAPTER  XVI \\nRELEVES  AND  ENTRIES  OF  POULTRY  AND  GAME    ....       473 \\nCHAPTER  XVII \\nROASTS  AND  SALADS         ........       605 \\nCHAPTER  XVIII \\nVEGETABLES  AND   FARINACEOUS  PRODUCTS  ....       624 \\nCHAPTER  XIX \\nSAVORIES       ..........       678 \\nCHAPTER  XX \\nENTREMETS.      (SWEETS)  .  .  .....       687 \\nCHAPTER  XXI \\nICES   AND  SHERBETS  .....  788 \\nCHAPTER  XXII \\nDRINKS   AND   REFRESHMENTS     .  .  .  .  .  .  .816 \\nCHAPTER  XXIII \\nFRUIT-STEWS  AND  JAMS  ,.,,...       820 \\n\"\n"
     ]
    }
   ],
   "source": [
    "doc = pymupdf.open(\"a guide to modern cookery.pdf\")\n",
    "toc_pages = [doc[20], doc[21]]\n",
    "toc_page_texts = []\n",
    "\n",
    "for page in toc_pages:\n",
    "    text = page.get_text()\n",
    "    toc_page_texts.append(text)\n",
    "    print(text.encode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FONDS  DE  CUISINE  \n",
      "  I \n",
      "THE  LEADING  WARM   SAUCES     \n",
      "  •  '5 \n",
      "THE   SMALL  COMPOUND   SAUCES  \n",
      "  24 \n",
      "COLD  SAUCES  AND  COMPOUND  BUTTERS        \n",
      "  48 \n",
      "SAVOURY  JELLIES  OR  ASPICS  \n",
      "  59 \n",
      "THE  COURT-BOUILLONS  AND  THE  MARINADES  \n",
      "  -64 \n",
      "\\J/:  ELEMENTARY  PREPARATIONS  \n",
      "  70 \n",
      "THE  VARIOUS  GARNISHES   FOR  SOUPS  \n",
      "  87 \n",
      "GARNISHING  PREPARATIONS   FOR  RELEVis  AND   ENTR]£eS  \n",
      "  92 \n",
      "U^DING  CULINARY  OPERATIONS  \n",
      "  97 \n",
      "HORS-D'CEUVRES      \n",
      "137 \n",
      "EGGS  \n",
      "        164 \n",
      "SOUPS  \n",
      "      197 \n",
      "FISH  \n",
      "        260 \n",
      "RELEVilS  AND  ENTRIES  OF  BUTCHER'S  MEAT  \n",
      "       352 \n",
      "RELEVES  AND  ENTRIES  OF  POULTRY  AND  GAME    \n",
      "       473 \n",
      "ROASTS  AND  SALADS         \n",
      "       605 \n",
      "VEGETABLES  AND   FARINACEOUS  PRODUCTS  \n",
      "       624 \n",
      "SAVORIES       \n",
      "       678 \n",
      "ENTREMETS\n",
      "       687 \n",
      "ICES   AND  SHERBETS  \n",
      "  788 \n",
      "DRINKS   AND   REFRESHMENTS     \n",
      "816 \n",
      "FRUIT-STEWS  AND  JAMS  ,\n",
      "       820 \n"
     ]
    }
   ],
   "source": [
    "chapters = {}\n",
    "\n",
    "for text in toc_page_texts:\n",
    "    lines = text.split(\"\\n\")\n",
    "    for i, line in enumerate(lines):\n",
    "        if i > 3:\n",
    "            if i%2 == 1:\n",
    "                # the title is the first words of the line \n",
    "                # followed by some amount of \".\"\n",
    "                # the page number is at the end\n",
    "                split_text = line.split(\".\")\n",
    "                title = split_text[0]\n",
    "                page_num = split_text[-1]\n",
    "                chapters[title] = page_num \n",
    "            \n",
    "for chapter in chapters:\n",
    "    print(chapter)\n",
    "    print(chapters[chapter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These chapters have errors:\n",
    "FONDS  DE  CUISINE  \n",
    "THE  LEADING  WARM   SAUCES     \n",
    "THE  COURT-BOUILLONS  AND  THE  MARINADES  \n",
    "\\J/:  ELEMENTARY  PREPARATIONS  \n",
    "GARNISHING  PREPARATIONS   FOR  RELEVis  AND   ENTR]£eS  \n",
    "U^DING  CULINARY  OPERATIONS  \n",
    "HORS-D'CEUVRES      \n",
    "RELEVilS  AND  ENTRIES  OF  BUTCHER'S  MEAT  \n",
    "RELEVES  AND  ENTRIES  OF  POULTRY  AND  GAME    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters['FONDS  DE  CUISINE  '] = 1\n",
    "chapters['THE  LEADING  WARM   SAUCES     '] = 15\n",
    "chapters['THE  COURT-BOUILLONS  AND  THE  MARINADES  '] = 64\n",
    "correct_titles = [\n",
    "    'THE  ELEMENTARY  PREPARATIONS  ',\n",
    "    'GARNISHING  PREPARATIONS   FOR  RELEVES  AND   ENTREES  ',\n",
    "    'LEADING  CULINARY  OPERATIONS  ',\n",
    "    \"HORS-D'OEUVRES      \",\n",
    "    \"RELEVES  AND  ENTREES  OF  BUTCHER'S  MEAT  \",\n",
    "    \"RELEVES  AND  ENTREES  OF  POULTRY  AND  GAME    \"  \n",
    "]\n",
    "incorrect_titles = [\n",
    "    '\\J/:  ELEMENTARY  PREPARATIONS  ',\n",
    "    'GARNISHING  PREPARATIONS   FOR  RELEVis  AND   ENTR]£eS  ',\n",
    "    'U^DING  CULINARY  OPERATIONS  ',\n",
    "    \"HORS-D'CEUVRES      \",\n",
    "    \"RELEVilS  AND  ENTRIES  OF  BUTCHER'S  MEAT  \",\n",
    "    \"RELEVES  AND  ENTRIES  OF  POULTRY  AND  GAME    \" \n",
    "]\n",
    "for i in range(len(correct_titles)):\n",
    "    chapters[correct_titles[i]] = chapters[incorrect_titles[i]]\n",
    "    del chapters[incorrect_titles[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can disregard the first 3 lines, and there are some minor issues with the text (e.g. \"U^DING\" instead of \"LEADING\"). Looking at the PDF, it seems there are some text marks on the original pages of the book that underwent OCR for the PDF. Since there are only a few mistakes, I'll fix these manually just to move forward. If this problem becomes too hindering I'll find a better method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the fixed titles and update the page numbers accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FONDS  DE  CUISINE  \n",
      "27\n",
      "THE  LEADING  WARM   SAUCES     \n",
      "41\n",
      "THE   SMALL  COMPOUND   SAUCES  \n",
      "50\n",
      "COLD  SAUCES  AND  COMPOUND  BUTTERS        \n",
      "74\n",
      "SAVOURY  JELLIES  OR  ASPICS  \n",
      "85\n",
      "THE  COURT-BOUILLONS  AND  THE  MARINADES  \n",
      "90\n",
      "THE  VARIOUS  GARNISHES   FOR  SOUPS  \n",
      "113\n",
      "EGGS  \n",
      "190\n",
      "SOUPS  \n",
      "223\n",
      "FISH  \n",
      "286\n",
      "ROASTS  AND  SALADS         \n",
      "631\n",
      "VEGETABLES  AND   FARINACEOUS  PRODUCTS  \n",
      "650\n",
      "SAVORIES       \n",
      "704\n",
      "ENTREMETS\n",
      "713\n",
      "ICES   AND  SHERBETS  \n",
      "814\n",
      "DRINKS   AND   REFRESHMENTS     \n",
      "842\n",
      "FRUIT-STEWS  AND  JAMS  ,\n",
      "846\n",
      "THE  ELEMENTARY  PREPARATIONS  \n",
      "96\n",
      "GARNISHING  PREPARATIONS   FOR  RELEVES  AND   ENTREES  \n",
      "118\n",
      "LEADING  CULINARY  OPERATIONS  \n",
      "123\n",
      "HORS-D'OEUVRES      \n",
      "163\n",
      "RELEVES  AND  ENTREES  OF  BUTCHER'S  MEAT  \n",
      "378\n",
      "RELEVES  AND  ENTREES  OF  POULTRY  AND  GAME    \n",
      "499\n"
     ]
    }
   ],
   "source": [
    "for chapter in chapters:\n",
    "    chapters[chapter] = int(chapters[chapter])+26  # 27-1 for zero indexing\n",
    "    print(chapter)\n",
    "    print(chapters[chapter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the recipes from Chapter 1, Fonds de Cuisine. After refining the method, then iterate and extract for every chapter. Will also write each extracted chapter to its own text file for future use to prevent having to run all of these cells in order to pick this up again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_nums = list(chapters.values())\n",
    "chapter_range = [page_nums[0], page_nums[1]]\n",
    "chapter_out = open(\"extracted chapters/chapter1.txt\",'wb')\n",
    "pages = []\n",
    "\n",
    "for i in range(chapter_range[0], chapter_range[1]):\n",
    "    page = doc[i]\n",
    "    pages.append(page)\n",
    "    text = page.get_text().encode(\"utf8\")\n",
    "    chapter_out.write(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By briefly examining the single chapter output file we see the following pattern for every recipe:\n",
    "- Title: \"3- CHICKEN CONSOMME\"\n",
    "- optionally: \"Quantities\" followed by the final amount and a list of ingredients\n",
    "- Instructions: \"Mode of Procedure\" or \"Preparation\"\n",
    "- Remarks: \"Remarks\"\n",
    "\n",
    "Occassionally there is a book title, page number, chapter title,  etc on its own line. Will remove these from the extracted text before extracting recipes via the patterns noted above.\n",
    "\n",
    "Lines for removal will contain:\n",
    "- \"GUIDE  TO  MODERN  COOKERY\"\n",
    "- Chapter title (e.g. \"FONDS DE CUISINE\")\n",
    "\n",
    "There are occasional typos. To handle the occasional typo, use fuzzy matching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_nums = list(chapters.values())\n",
    "titles = list(chapters.keys())\n",
    "chapter_range = [page_nums[0], page_nums[1]]\n",
    "chapter_out = open(\"extracted chapters/chapter1.txt\",'wb')\n",
    "pages = []\n",
    "\n",
    "target_phrases = [\"GUIDE TO MODERN COOKERY\", titles[0]]\n",
    "\n",
    "for i in range(chapter_range[0], chapter_range[1]):\n",
    "    page = doc[i]\n",
    "    pages.append(page)\n",
    "    text = page.get_text()\n",
    "    lines = text.splitlines()\n",
    "    filtered_lines = [\n",
    "        line for line in lines\n",
    "        if not any(fuzz.partial_ratio(line.strip(), phrase) > 80 for phrase in target_phrases)\n",
    "    ]\n",
    "    chapter_out.write(\"\\n\".join(filtered_lines).encode(\"utf8\") + b\"\\n\")\n",
    "\n",
    "chapter_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that most of the inconsequential lines have been removed, the recipes need to be separated and parsed for relevant information. \n",
    "Following the earlier idenitification of the recipe structure, a possible format for a JSONified recipe could be:\n",
    "{\n",
    "    \"recipe_id\": 1,\n",
    "    \"title\": \"Ordinary or White Consomme\"\n",
    "    \"quantities\": {\n",
    "        \"final_amount\": \"4 quarts\",\n",
    "        \"ingredients\": [\n",
    "            \"3 lbs. of shin of beef\",\n",
    "            \"3 lbs. of lean beef\",\n",
    "            ...\n",
    "        ]\n",
    "    },\n",
    "    \"instructions\": \"Preparation - Put the emeat into a stock-pot...\",\n",
    "    \"remarks\": \"Remarks Relative to...\"\n",
    "}\n",
    "\n",
    "Note: Since some recipes do not have all of these fields, they will be set to None or omitted entirely. \n",
    "Further note: Upon further reading and skipping through the book, many of the chapters have only a title and instructions with ingredients spread throughout. The recipe format will instead have \"id\", \"title\", and \"instructions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_nums = list(chapters.values())\n",
    "titles = list(chapters.keys())\n",
    "chapter_range = [page_nums[0], page_nums[1]]\n",
    "all_recipes = {}\n",
    "\n",
    "n = 0\n",
    "for i in range(0, len(page_nums) - 1, 2):\n",
    "    chapter_range = [page_nums[i], page_nums[i+1]]\n",
    "    pages = []\n",
    "    recipes = []\n",
    "\n",
    "    # filtering out the title and chapter title lines\n",
    "    target_phrases = [\"GUIDE TO MODERN COOKERY\", titles[n]]\n",
    "    n += 1\n",
    "    for j in range(chapter_range[0], chapter_range[1]):\n",
    "        page = doc[j]\n",
    "        text = page.get_text()\n",
    "        lines = text.splitlines()\n",
    "        filtered_lines = [\n",
    "            line for line in lines\n",
    "            if not any(fuzz.partial_ratio(line.strip(), phrase) > 80 for phrase in target_phrases)\n",
    "        ]\n",
    "        pages.append(\"\\n\".join(filtered_lines))\n",
    "    chapter_text = \"\\n\".join(pages)\n",
    "\n",
    "    # extracting recipes\n",
    "    # extract number and title\n",
    "    recipe_pattern = re.compile(\n",
    "        r'(?P<header>[A-Za-z0-9]{1,4}—\\s+.+?)(?=\\n[A-Za-z0-9]{1,4}—|\\Z)',\n",
    "    re.DOTALL)  # 1 to 4 character identifiers,the em dash, spaces, look ahead to stop capturing when another identifier is found\n",
    "\n",
    "    for block in recipe_pattern.finditer(chapter_text):\n",
    "        block_text = block.group(0) # extract whole match\n",
    "        current_recipe = {}\n",
    "\n",
    "        # extract title and id\n",
    "        header_patter = re.compile(\n",
    "            r'^(?P<id>[A-Za-z0-9]{1,4})—(?P<title>.*)$',\n",
    "            re.MULTILINE\n",
    "        )\n",
    "        header_match = header_patter.search(block_text)\n",
    "        if header_match:\n",
    "            current_recipe[\"recipe_id\"] = header_match.group(\"id\")\n",
    "            current_recipe[\"title\"] = header_match.group(\"title\").strip()\n",
    "            # extract everything until the next recipe match and add to current_recipe as \"instructions\"\n",
    "            current_recipe[\"instructions\"] = block_text[header_match.end():].strip().replace(\"\\n\",\"\")\n",
    "        else:\n",
    "            # if no header is found, just store everything as instructions\n",
    "            current_recipe[\"instructions\"] = block_text.strip().replace(\"\\n\", \"\")\n",
    "\n",
    "        # add to list of recipes for this chapter\n",
    "        recipes.append(current_recipe)\n",
    "    \n",
    "    # add list of this chapter's recipes to the dictionary of recipes by chapter\n",
    "    all_recipes[titles[n-1]] = recipes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save each chapter of JSONified recipes to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"json_chapters\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for chapter, recipe in all_recipes.items():\n",
    "    # ensure the chapter title can be a file name\n",
    "    # safe_chapter = slugify(chapter)\n",
    "    filename = f\"{chapter}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    with open(filepath, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(recipes, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps:\n",
    "1. Preprocess and clean the extracted recipes further\n",
    "    - ensure its clean, remove unwanted formatting, handle line breaks, and normalize where necessary (like instructions.)\n",
    "2. Segment the text into meaningful chunks (Optionally?)\n",
    "    - to improve granularity as needed to help the model retrieve the most relevant parts\n",
    "3. Compute embeddings\n",
    "    - Generate embeddings for each text chunk and link it with its associated metadata.\n",
    "    - Ollama: https://ollama.com/blog/embedding-models \n",
    "    - Relevant Reddit Thread: https://www.reddit.com/r/LocalLLaMA/comments/17oyd1r/finding_better_embedding_models/\n",
    "    - Embedding Leaderboard: https://huggingface.co/spaces/mteb/leaderboard\n",
    "    - Will likely use either `bge-large-en-v1.5` or `e5-large-v2` or `mxbai-embed-large-v1`, best for my purpose currently seems to be e5\n",
    "4. Index with a Vector db \n",
    "    - FAISS, Pinecone,  Weaviate, or Milvus\n",
    "    - insert each embedding with its metadata\n",
    "    - configure similarity parameters\n",
    "5. Integrate with the LLM\n",
    "    - Embed the query with the embed model from earlier, then perform similarity search in the vector db. Use whatever is retrieved as context for the LLM.\n",
    "6. Prototype, Test, and Iterate\n",
    "    - Experiment with the various parameters\n",
    "7. Make a Gradio Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
